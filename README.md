# Kit de In√≠cio para IA Auto-hospedada

**Self-hosted AI Starter Kit** √© um modelo de Docker Compose de c√≥digo aberto projetado para inicializar rapidamente um ambiente abrangente de desenvolvimento de IA e low-code local.

![n8n.io - Captura de Tela](https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif)

Criado por <https://github.com/n8n-io>, ele combina a plataforma n8n auto-hospedada com uma lista selecionada de produtos e componentes de IA compat√≠veis para come√ßar rapidamente a criar fluxos de trabalho de IA auto-hospedados.

> [!DICA]
> [Leia o an√∫ncio](https://blog.n8n.io/self-hosted-ai/)

### O que est√° inclu√≠do

‚úÖ [**n8n auto-hospedado**](https://n8n.io/) - Plataforma low-code com mais de 400 integra√ß√µes e componentes avan√ßados de IA

‚úÖ [**Ollama**](https://ollama.com/) - Plataforma LLM multiplataforma para instalar e executar os mais recentes modelos de linguagem locais

‚úÖ [**Qdrant**](https://qdrant.tech/) - Armazenamento vetorial de alto desempenho com uma API abrangente

‚úÖ [**PostgreSQL**](https://www.postgresql.org/) - O cavalo de batalha do mundo de Engenharia de Dados, lida com grandes quantidades de dados com seguran√ßa.

‚úÖ [**Open WebUI**](https://github.com/open-webui/open-webui) - Interface web amig√°vel para interagir com LLMs locais atrav√©s do Ollama

### O que voc√™ pode construir

‚≠êÔ∏è **Agentes de IA** para agendamento de compromissos

‚≠êÔ∏è **Resumir PDFs da Empresa** com seguran√ßa sem vazamento de dados

‚≠êÔ∏è **Bots do Slack mais inteligentes** para melhorar as comunica√ß√µes e opera√ß√µes de TI da empresa

‚≠êÔ∏è **An√°lise de Documentos Financeiros Privados** com custo m√≠nimo

## Instala√ß√£o

### Clonando o Reposit√≥rio

```bash
git clone https://github.com/fabriciosilvasantos/self-hosted-ai-starter-kit.git
cd self-hosted-ai-starter-kit
cp .env.example .env # voc√™ deve atualizar os segredos e senhas dentro
```

### Executando n8n usando Docker Compose

#### Para usu√°rios com GPU Nvidia

```bash
git clone https://github.com/fabriciosilvasantos/self-hosted-ai-starter-kit.git
cd self-hosted-ai-starter-kit
cp .env.example .env # voc√™ deve atualizar os segredos e senhas dentro
docker compose --profile gpu-nvidia up
```

> [!NOTA]
> Se voc√™ nunca usou sua GPU Nvidia com o Docker antes, siga as
> [instru√ß√µes do Docker da Ollama](https://github.com/ollama/ollama/blob/main/docs/docker.md).

### Para usu√°rios com GPU AMD no Linux

```bash
git clone https://github.com/fabriciosilvasantos/self-hosted-ai-starter-kit.git
cd self-hosted-ai-starter-kit
cp .env.example .env # voc√™ deve atualizar os segredos e senhas dentro
docker compose --profile gpu-amd up
```

#### Para usu√°rios de Mac / Apple Silicon

Se voc√™ estiver usando um Mac com processador M1 ou mais recente, infelizmente n√£o √© poss√≠vel expor sua GPU para a inst√¢ncia do Docker. Existem duas op√ß√µes neste caso:

1. Execute o kit de in√≠cio totalmente na CPU, como na se√ß√£o "Para todos os outros" abaixo
2. Execute o Ollama no seu Mac para infer√™ncia mais r√°pida e conecte-se a ele a partir da inst√¢ncia do n8n

Se voc√™ quiser executar o Ollama no seu Mac, verifique a [p√°gina inicial do Ollama](https://ollama.com/) para obter instru√ß√µes de instala√ß√£o e execute o kit de in√≠cio da seguinte forma:

```bash
git clone https://github.com/fabriciosilvasantos/self-hosted-ai-starter-kit.git
cd self-hosted-ai-starter-kit
cp .env.example .env # voc√™ deve atualizar os segredos e senhas dentro
docker compose up
```

##### Para usu√°rios de Mac executando OLLAMA localmente

Se voc√™ estiver executando o OLLAMA localmente no seu Mac (n√£o no Docker), voc√™ precisa modificar a vari√°vel de ambiente OLLAMA_HOST

1. Defina OLLAMA_HOST como `host.docker.internal:11434` no seu arquivo .env.
2. Al√©m disso, depois de ver "Editor is now accessible via: <http://localhost:5678/>":

    1. Acesse <http://localhost:5678/home/credentials>
    2. Clique em "Local Ollama service"
    3. Altere a URL base para "http://host.docker.internal:11434/"

#### Para todos os outros

```bash
git clone https://github.com/fabriciosilvasantos/self-hosted-ai-starter-kit.git
cd self-hosted-ai-starter-kit
cp .env.example .env # voc√™ deve atualizar os segredos e senhas dentro
docker compose --profile cpu up
```

## ‚ö°Ô∏è In√≠cio r√°pido e uso

O n√∫cleo do Self-hosted AI Starter Kit √© um arquivo Docker Compose, pr√©-configurado com configura√ß√µes de rede e armazenamento, minimizando a necessidade de instala√ß√µes adicionais.
Ap√≥s concluir as etapas de instala√ß√£o acima, basta seguir as etapas abaixo para come√ßar.

1. Abra <http://localhost:5678/> no seu navegador para configurar o n8n. Voc√™ s√≥ precisar√° fazer isso uma vez.
2. Abra o fluxo de trabalho inclu√≠do: <http://localhost:5678/workflow/srOnR8PAY3u4RSwb>
3. Clique no bot√£o **Chat** na parte inferior da tela para come√ßar a executar o fluxo de trabalho.
4. Se esta for a primeira vez que voc√™ executa o fluxo de trabalho, pode ser necess√°rio aguardar at√© que o Ollama termine de baixar o Llama3.2. Voc√™ pode inspecionar os logs do console do docker para verificar o progresso.

Para abrir o n8n a qualquer momento, visite <http://localhost:5678/> no seu navegador.

Com sua inst√¢ncia do n8n, voc√™ ter√° acesso a mais de 400 integra√ß√µes e um conjunto de n√≥s de IA b√°sicos e avan√ßados, como [Agente de IA](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/), [Classificador de Texto](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/) e [Extrator de Informa√ß√µes](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/). Para manter tudo local, lembre-se de usar o n√≥ Ollama para seu modelo de linguagem e Qdrant como seu armazenamento vetorial.

> [!NOTA]
> Este kit de in√≠cio foi projetado para ajud√°-lo a come√ßar com fluxos de trabalho de IA auto-hospedados. Embora n√£o seja totalmente otimizado para ambientes de produ√ß√£o, ele combina componentes robustos que funcionam bem juntos para projetos de prova de conceito. Voc√™ pode personaliz√°-lo para atender √†s suas necessidades espec√≠ficas.

## Atualiza√ß√£o

* ### Para configura√ß√µes com GPU Nvidia:

```bash
docker compose --profile gpu-nvidia pull
docker compose create && docker compose --profile gpu-nvidia up
```

* ### Para usu√°rios de Mac / Apple Silicon

```bash
docker compose pull
docker compose create && docker compose up
```

* ### Para configura√ß√µes sem GPU:

```bash
docker compose --profile cpu pull
docker compose create && docker compose --profile cpu up
```

## üëì Leitura recomendada

O n8n est√° repleto de conte√∫do √∫til para come√ßar rapidamente com seus conceitos e n√≥s de IA. Se voc√™ encontrar algum problema, v√° para [suporte](#suporte).

- [Agentes de IA para desenvolvedores: da teoria √† pr√°tica com n8n](https://blog.n8n.io/ai-agents/)
- [Tutorial: Crie um fluxo de trabalho de IA no n8n](https://docs.n8n.io/advanced-ai/intro-tutorial/)
- [Conceitos de Langchain no n8n](https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/)
- [Demonstra√ß√£o das principais diferen√ßas entre agentes e cadeias](https://docs.n8n.io/advanced-ai/examples/agent-chain-comparison/)
- [O que s√£o bancos de dados vetoriais?](https://docs.n8n.io/advanced-ai/examples/understand-vector-databases/)

## üé• Demonstra√ß√£o em v√≠deo

- [Instalando e usando IA Local para n8n](https://www.youtube.com/watch?v=xz_X2N-hPg0)

## üõçÔ∏è Mais modelos de IA

Para mais ideias de fluxos de trabalho de IA, visite a [galeria oficial de modelos de IA do n8n](https://n8n.io/workflows/categories/ai/). Em cada fluxo de trabalho, selecione o bot√£o **Usar fluxo de trabalho** para importar automaticamente o fluxo de trabalho para sua inst√¢ncia local do n8n.

### Modelos de IA local

- [Assistente de C√≥digo Tribut√°rio](https://n8n.io/workflows/2341-build-a-tax-code-assistant-with-qdrant-mistralai-and-openai/)
- [Dividir Documentos em Notas de Estudo com MistralAI e Qdrant](https://n8n.io/workflows/2339-breakdown-documents-into-study-notes-using-templating-mistralai-and-qdrant/)
- [Assistente de Documentos Financeiros usando Qdrant e Mistral.ai](https://n8n.io/workflows/2335-build-a-financial-documents-assistant-using-qdrant-and-mistralai/)
- [Recomenda√ß√µes de Receitas com Qdrant e Mistral](https://n8n.io/workflows/2333-recipe-recommendations-with-qdrant-and-mistral/)

## Dicas e truques

### Acessando arquivos locais

O kit de in√≠cio de IA auto-hospedada criar√° uma pasta compartilhada (por padr√£o, localizada no mesmo diret√≥rio) que √© montada no cont√™iner n8n e permite que o n8n acesse arquivos em disco. Esta pasta dentro do cont√™iner n8n est√° localizada em `/data/shared` - este √© o caminho que voc√™ precisar√° usar em n√≥s que interagem com o sistema de arquivos local.

**N√≥s que interagem com o sistema de arquivos local**

- [Ler/Escrever Arquivos do Disco](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.filesreadwrite/)
- [Gatilho de Arquivo Local](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/)
- [Executar Comando](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executecommand/)

## üìú Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa Apache 2.0 - consulte o arquivo [LICENSE](LICENSE) para obter detalhes.

## üí¨ Suporte

Participe da conversa no [F√≥rum do n8n](https://community.n8n.io/), onde voc√™ pode:

- **Compartilhar Seu Trabalho**: Mostre o que voc√™ construiu com o n8n e inspire outras pessoas na comunidade.
- **Fazer Perguntas**: Seja voc√™ um iniciante ou um profissional experiente, a comunidade e nossa equipe est√£o prontas para ajudar com quaisquer desafios.
- **Propor Ideias**: Tem uma ideia para um recurso ou melhoria? Conte para n√≥s! Estamos sempre ansiosos para ouvir o que voc√™ gostaria de ver em seguida.
